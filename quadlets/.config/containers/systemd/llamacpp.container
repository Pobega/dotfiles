[Unit]
Description=Llama.cpp Server

[Container]
ContainerName=llamacpp
Image=ghcr.io/ggml-org/llama.cpp:server-vulkan
Exec=--models-dir /models --models-max 1 -c 32768 -ngl 30 --port 9090
Pull=newer
Volume=/var/home/pobega/models:/models
AddDevice=/dev/kfd
AddDevice=/dev/dri
PublishPort=9090:9090
SecurityLabelType=container_runtime_t
Environment=LLAMA_ARG_HOST=0.0.0.0

[Service]
Restart=always

[Install]
WantedBy=multi-user.target default.target
